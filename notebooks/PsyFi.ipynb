{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852ca08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Processor:\n",
    "    '''\n",
    "    Class to read and flag anonymous transaction\n",
    "    based on historical data analysis.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame)-> pd.DataFrame:\n",
    "        '''\n",
    "        load and log transform data.\n",
    "        '''\n",
    "        self.data= data\n",
    "        self.data.drop(columns='Unnamed: 0', inplace= True)\n",
    "        self.data.set_index('timestamp', drop= False, inplace= True)\n",
    "        self.data['amount'] = np.log(self.data['amount'])\n",
    "        self.data['timestamp'] = pd.to_datetime(self.data['timestamp'])\n",
    "        self.data['month']= self.data['timestamp'].dt.month\n",
    "        self.data['week']= np.int64(self.data['timestamp'].dt.strftime('%U'))\n",
    "        self.data['day']= self.data['timestamp'].dt.day\n",
    "        self.data['week_day']= self.data['timestamp'].dt.weekday\n",
    "        self.data['binned_hour'] = self.data['transaction_hour']//3\n",
    "        self.data['timestamp_lag'] = self.data['timestamp'].shift(1)\n",
    "        self.data['time_diff'] = self.data['timestamp']-self.data['timestamp_lag']\n",
    "        self.data['time_diff'] = self.data['time_diff'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "        # Estimate average no. of transactions per day\n",
    "        self.freq_per_day= np.int64(self.data.groupby(['day'])['day'].count().mean())\n",
    "    \n",
    "    def calculate_z_score(self, value: np.array , mean: np.float64, std: np.float64, \n",
    "                          att_name: str)-> tuple:\n",
    "        '''\n",
    "        Calculates z-score for the value\n",
    "        using given mean, standard deviation\n",
    "        '''\n",
    "        z = (value - mean) / std\n",
    "        stat=  f'Mean and Standard deviation for {att_name} are {mean}, {std} respectively.'\n",
    "        return z, stat\n",
    "    \n",
    "    def calculate_rolling_stats(self, data: pd.Series, window: np.int64)-> tuple:\n",
    "        '''\n",
    "        Use sliding window to smoothen data and get approximate mean \n",
    "        and standard deviation over the time\n",
    "        '''\n",
    "        rolling_mean= data.rolling(window, min_periods= 1).mean()\n",
    "        rolling_std= data.rolling(window, min_periods = 1).std()\n",
    "        avg= np.nanmean(rolling_mean)\n",
    "        std= rolling_std.values[-1]\n",
    "        stat= f'The rolling avg and standard deviation of amount in last {window} transactions are {avg}, {std} respectively.'\n",
    "        return ((avg, std), stat)\n",
    "    \n",
    "    def flag_freq_volume(self, data: pd.DataFrame, timestamp: datetime, hour_bin: np.int64, amount: np.float64):\n",
    "        '''\n",
    "        Function to claculate velocity of transaction\n",
    "        to identify unusual frequent transaction of small amounts\n",
    "        during specific hour segment in comaprision with hostoric data.\n",
    "        '''\n",
    "\n",
    "        # Group by day and binned hour to understand frequency, size of transaction \n",
    "        # each day during the target hour.\n",
    "        velo_df= data.groupby(['day','binned_hour'], as_index= False).agg(avg_amount=('amount', 'mean'), avg_time_diff= ('time_diff', 'mean'))\n",
    "        velo_df[['day', 'binned_hour']]= velo_df[['day', 'binned_hour']].astype('int32')\n",
    "\n",
    "        # Time elapsed betweeen current and last transaction.\n",
    "        time_delta= timestamp - data.loc[data.index.max(), 'timestamp']\n",
    "        time_delta= time_delta.total_seconds()\n",
    "\n",
    "        # Z-score for the time difference  \n",
    "        time_avg= velo_df['avg_time_diff'].mean()\n",
    "        time_std= velo_df['avg_time_diff'].std()\n",
    "        Z_time, _= self.calculate_z_score(time_delta, time_avg, time_std, 'Time Differance')\n",
    "\n",
    "        # Z-score for the amount \n",
    "        amount_avg= velo_df['avg_amount'].mean()\n",
    "        amount_std= velo_df['avg_amount'].std()\n",
    "        Z_amount, _= self.calculate_z_score(amount, amount_avg, amount_std, 'Time Differance Amount')\n",
    "\n",
    "        # High frequency low amount transactions\n",
    "        freq_violation = Z_time < -2 # high frequency\n",
    "        amount_violation = Z_amount < -3 # low amount\n",
    "\n",
    "        freq_vol_stats= f'Average & standard deviation for time differance and amount spent during binned hour {hour_bin} are\\\n",
    "                        ({time_avg}, {time_std}), ({amount_avg}, {amount_std}) respectively.'\n",
    "\n",
    "        return ((freq_violation & amount_violation), freq_vol_stats)\n",
    "    \n",
    "    def calculate_stats(self, json_input):\n",
    "        '''\n",
    "        Main function to which the OCR data is passed, it\n",
    "        further used other class methods in conjunction to \n",
    "        decide on normality of the transaction using statistics.\n",
    "        '''\n",
    "        try:\n",
    "            total_amount= np.log(np.float64(json_input['amount']).item())\n",
    "            timestamp= json_input.get('timestamp', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            timestamp= pd.to_datetime(timestamp)\n",
    "            \n",
    "            week= timestamp.week\n",
    "            # month= timestamp.month\n",
    "            # week_day= timestamp.day_of_week\n",
    "            hour= timestamp.hour\n",
    "            hour_bin= hour//3\n",
    "\n",
    "            merchant= json_input.get('merchant', 'Unknown').lower()\n",
    "            merchant_cat= json_input.get('merchant_category', 'Unknown').lower()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(json_input)\n",
    "            raise e\n",
    "\n",
    "        window = self.freq_per_day\n",
    "\n",
    "        # Rolling window stats for last 24 hours (136 previous data points), using week to get enough data\n",
    "        prior_week= week\n",
    "        week_data= pd.DataFrame()\n",
    "        while week_data.shape[0] < window:\n",
    "            week_data= self.data[self.data['week'] >= prior_week]\n",
    "            prior_week= prior_week-1\n",
    "\n",
    "        rolling_mean, rolling_std= self.calculate_rolling_stats(week_data['amount'], window= window)[0]\n",
    "\n",
    "        # Hourly stats since past few weeks\n",
    "        bin_hour_mean= week_data[(week_data['binned_hour'] == hour_bin)]['amount'].mean()\n",
    "        bin_hour_std= week_data[(week_data['binned_hour'] == hour_bin)]['amount'].std()\n",
    "\n",
    "        # Category Level stats\n",
    "        merchant_cat_mean= week_data[(week_data['merchant_category'].str.lower() == merchant_cat)]['amount'].mean()\n",
    "        merchant_cat_std= week_data[(week_data['merchant_category'].str.lower() == merchant_cat)]['amount'].std()\n",
    "\n",
    "        # Merchant Level stats\n",
    "        merchant_mean= week_data[(week_data['merchant'].str.lower() == merchant)]['amount'].mean()\n",
    "        merchant_std= week_data[(week_data['merchant'].str.lower() == merchant)]['amount'].std()\n",
    "        \n",
    "        # # Hourly stats since past few weeks\n",
    "        # bin_hour_mean= self.data[(self.data['binned_hour'] == hour_bin) & (self.data['week'] >= prior_week)]['amount'].mean()\n",
    "        # bin_hour_std= self.data[(self.data['binned_hour'] == hour_bin) & (self.data['week'] >= prior_week)]['amount'].std()\n",
    "\n",
    "        # # Category Level stats\n",
    "        # merchant_cat_mean= self.data[(self.data['merchant_category'].str.lower() == merchant_cat) & (self.data['week'] >= prior_week)]['amount'].mean()\n",
    "        # merchant_cat_std= self.data[(self.data['merchant_category'].str.lower() == merchant_cat) & (self.data['week'] >= prior_week)]['amount'].std()\n",
    "\n",
    "        # # Merchant Level stats\n",
    "        # merchant_mean= self.data[(self.data['merchant'].str.lower() == merchant) & (self.data['week'] >= prior_week)]['amount'].mean()\n",
    "        # merchant_std= self.data[(self.data['merchant'].str.lower() == merchant) & (self.data['week'] >= prior_week)]['amount'].std()\n",
    "\n",
    "        # Frequency Volume stats\n",
    "        hig_freq_low_volume_flag, hig_freq_low_volume_stats = self.flag_freq_volume(week_data, timestamp, hour_bin, total_amount)\n",
    "\n",
    "        z_rolling_amount = self.calculate_z_score(total_amount, rolling_mean, rolling_std, 'Z_Rolling_Amount')\n",
    "        z_bin_hr_amount = self.calculate_z_score(total_amount, bin_hour_mean, bin_hour_std, 'Z_Bin_hour_Amount')\n",
    "        z_merchant_cat_amount = self.calculate_z_score(total_amount, merchant_cat_mean, merchant_cat_std, 'Z_Merchant_Cat_Amount')\n",
    "        z_merchant_amount = self.calculate_z_score(total_amount, merchant_mean, merchant_std, 'Z_Merchant_Amount')\n",
    "\n",
    "        return ((z_rolling_amount, z_bin_hr_amount, z_merchant_cat_amount, z_merchant_amount), (hig_freq_low_volume_flag, hig_freq_low_volume_stats)) \n",
    "    \n",
    "data_ = pd.read_csv('synthetic_transactions_v1.csv')\n",
    "\n",
    "in_file= {\n",
    "    'merchant': 'Amazon Prime',\n",
    "    'merchant_category': 'Entertainment',\n",
    "    'amount': 5.484718,\n",
    "    'timestamp': '2024-10-30 23:19:29.399558+00:00'\n",
    "}\n",
    "\n",
    "obj= Data_Processor(data_)\n",
    "stat= obj.calculate_stats(in_file)\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d81e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((np.float64(-2.960851255431856),\n",
       "   'Mean and Standard deviation for Z_Rolling_Amount are 8.315793414073323, 2.2337588632589216 respectively.'),\n",
       "  (np.float64(-2.6082296764435124),\n",
       "   'Mean and Standard deviation for Z_Bin_hour_Amount are 8.03898235841031, 2.4296237161100502 respectively.'),\n",
       "  (np.float64(-2.961645294074422),\n",
       "   'Mean and Standard deviation for Z_Merchant_Cat_Amount are 8.035465685040307, 2.1385072744028766 respectively.'),\n",
       "  (np.float64(-0.8951269253266472),\n",
       "   'Mean and Standard deviation for Z_Merchant_Amount are 6.307298740447541, 5.144893903516383 respectively.')),\n",
       " (np.True_,\n",
       "  'Average & standard deviation for time differance and amount spent during binned hour 7 are                        (800.1326823314521, 315.1133412203902), (8.240697748588087, 0.48021174151273993) respectively.'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ = pd.read_csv('synthetic_transactions_v1.csv')\n",
    "\n",
    "in_file= {\n",
    "    'merchant': 'Amazon Prime',\n",
    "    'merchant_category': 'Entertainment',\n",
    "    'amount': 5.484718,\n",
    "    'timestamp': '2024-10-30 23:19:29.399558+00:00'\n",
    "}\n",
    "\n",
    "obj= Data_Processor(data_)\n",
    "stat= obj.calculate_stats(in_file)\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ee266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ff01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uvicorn :app --reload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
